\section{Greedy and Lookahead Policies}

%%PA: -> "Greedy Policy and Lookahead Policies"

%%PA: how about cutting to the point more directly and leading off with: "The solution to the (convex) optimization
%%problem described in section~\ref{...} provides us with a weight
%%vector \weights{}, which defines ...

%Given an appropriate choice of \featurefn{}, we can set up an optimization
%problem based on the formulation outlined in
%Section~\ref{sec:formulation}. Solving this optimization problem produces a
%weight vector \weights{}, which defines an approximate Q-function:

The solution to the (convex) optimization problem described in 
Section~\ref{sec:formulation} provides us with a weight vector \weights{}, which defines 
an approximate Q-function:
\begin{equation}
  \approxq(s,a) = \weightst \features{s}{a}
\end{equation}

Once we obtain \approxq{}, the most natural choice of policy is the
simple greedy policy, which always selects the action that yields the highest
approximate Q-value:

\begin{equation}
  \policyvar(s) = \argmax_{\actionvar \in \actionset} \approxq(s,a)
\end{equation}

In the case where \approxq{} exactly represents the true Q-function of the MDP,
\policyvar{} is an optimal policy. However, since the constraints in the optimization
problem are only a subset of the full set of constraints imposed by the MDP, and
the approximate Q-function is simply a linear combination of the features, in
practice \approxq{} serves as only a loose approximation of the true function
$Q$. Thus, other choices of \policyvar{} instead of the na\"{\i}ve greedy
policy have the potential to improve the robustness of action selection.

One such strategy for determining a policy takes advantage of the fact that
learning an approximate Q-function confers the ability to compare the value of
states directly. If we allow for simulation of the task in question, then we can
use \approxq{} as a heuristic to look ahead to promising states.

One way of performing this lookahead is through beam search. Beam search defines
a family of policies based on a particular \approxq{}, in which each policy is
characterized by a lookahead width $w_\ell$ and depth $d_\ell$. To select an action under
one of these policies, we repeatedly simulate the $w_\ell$ most promising actions up
to a depth of $d_\ell$, then select the initial action which yields the successor
with the highest $\tilde{V}$, where $\tilde{V}$ is defined as

\begin{equation}
  \tilde{V}(\statevar) = \max_{\actionvar \in \actionset} \approxq(\statevar, \actionvar).
\end{equation}


If at any point during this expansion we encounter a goal state, we select the
action that leads to it---this is equivalent to enforcing
$\tilde{V}(\statevar{}) > \tilde{V}(\statevar{}')$ for $\statevar{} \in
\goalset{}, \statevar{}' \not\in \goalset{}$.

We evaluate the effectiveness of the greedy policy and the family of beam search
lookahead policies in Section~\ref{sec:experiments}.
