\section{Greedy and Lookahead Policies}

The solution to the (convex) optimization problem described in 
Section~\ref{sec:formulation} provides us with a weight vector \weights{}, which defines 
an approximate Q-function:
\begin{equation}
  \approxq(s,a) = \weightst \features{s}{a}
\end{equation}

Once we obtain \approxq{}, the most natural choice of policy is the
simple greedy policy, which always selects the action that yields the highest
approximate Q-value:

\begin{equation}
  \policyvar(s) = \argmax_{\actionvar \in \actionset} \approxq(s,a)
\end{equation}

In the case where \approxq{} exactly represents the true Q-function of the MDP,
\policyvar{} is an optimal policy. However, since the constraints in the optimization
problem are only a subset of the full set of constraints imposed by the MDP, and
the approximate Q-function is simply a linear combination of the features, in
practice \approxq{} serves as only a loose approximation of the true function
$Q$. Thus, other choices of \policyvar{} instead of the na\"{\i}ve greedy
policy have the potential to improve the robustness of action selection.

One such strategy for determining a policy takes advantage of the fact that
learning an approximate Q-function confers the ability to compare the value of
states directly. If we allow for simulation of the task in question, then we can
use \approxq{} as a heuristic to look ahead to promising states. This method of
value function approximation paired with lookahead is a standard approach to
determining a policy.
% TODO: Add a citation at the end of this paragraph

One way of performing this lookahead is through beam search. Beam search defines
a family of policies based on a particular \approxq{}, in which each policy is
characterized by a lookahead width $w_\ell$ and depth $d_\ell$. To select an action under
one of these policies, we repeatedly simulate the $w_\ell$ most promising actions up
to a depth of $d_\ell$, then select the initial action which yields the successor
with the highest $\tilde{V}$, where $\tilde{V}$ is defined as

\begin{equation}
  \tilde{V}(\statevar) = \max_{\actionvar \in \actionset} \approxq(\statevar, \actionvar).
\end{equation}


If at any point during this expansion we encounter a goal state, we select the
action that leads to it---this is equivalent to enforcing
$\tilde{V}(\statevar{}) > \tilde{V}(\statevar{}')$ for $\statevar{} \in
\goalset{}, \statevar{}' \not\in \goalset{}$.

We use a knot-tying simulation environment to evaluate the effectiveness of the
greedy policy and the family of beam search lookahead policies in
Section~\ref{sec:experiments}.
