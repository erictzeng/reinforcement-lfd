\section{Related Work}
Related work for our contribution stems from three areas of research: deformable object manipulation (in particular knot-tieing), learning from demonstrations, and hierarchical reinforcement learing.
\dhm{I took out sub section headings here b/c they looked a little wonky. Not super attached to that though.}


%\subsection{Learning from Demonstrations}
The problem of learning from demonstrations deals with the generalization of expert demonstrations to new scenarios. 
This is a broad area of research with many different approaches and broad applicabilty~\cite{Argall_2009, Schaal_1999}.
The approach to learning from demonstrations applied here is most closely linked to behavioral cloning; where an agent atempts to learn a function that directly maps states to actions so as to mimic the experts behavior. 

One of the first successful applications of this strategy was the ALVINN system~\cite{Pomerleau_NIPS1989}, which utilized a neural network to learning a policy such that an autonomous car could follow a road.
In this a function is learned that can map image input and range finder input to left and right steering commands. 
This research direction has been furthered recently by \citet{muller2005off}, who use a convolutional network to learn a steering policy for off-road driving.
\citet{Ross_2013} propose a method to directly control a Micro UAV from RGB camera input and describe a method to account for differences in the training distribution under expert demonstration and the induced distribution under a learned policy. 
These approaches differ from the proposed method in that they do learning for low-dimensional action spaces and directly imitate the expert.
In contrast, MMQL copes with high dimensional state and action spaces as well as accounting for temporal constraints from demonstrated behavior.

Miyamoto et al. describe an approach for learning to play Kendama~\cite{Miyamoto_1996} and hit a tennis ball~\cite{Miyamoto_1998} from demonstrated actions. 
This method was successful at generalizing a human trajectory to a robot and incorporates the sequential combination of multiple demonstrations.
However, their approach requires a hand tuning of waypoints and does not generalize to new scenes.


The most similar behavioral cloning working to our approach is that of \citet{Isaac_ICML2003}.
They use behavioral cloning to learn to fly an airplane.
They make use of an abstract goal directed layer, which sits on top of a low-level PID controller.
This goal directed learning is similar in spirit to ours, although it makes use of a different formalism.
The primary difference with this work is in the complexity of the low level controllers and complexity derived from manipulation.

Similarly to our approach, \citet{Dvijotham_ICML2010} also attempt to directly
learn a value function or Q-function for a MDP given sample transitions
generated by an optimal control policy. However, they assume either a discrete
state space or a linear dynamics model. In contrast, our method makes no
assumptions about the size of the state space or the dynamics model, instead
relying on segments of expert demonstrations as a means of navigating the state
space efficiently. \et{Someone who actually understands robotics should probably
  check this statement.}

%\subsection{Deformable Object Manipulation}
Our approach can be applied towards a variety of tasks in robotics,
including the manipulation of deformable objects.
In particular, we demonstrate the effectiveness of our approach for
knot tying, a commonly studied manipulation task in robotics.
Previous approaches to knot tying usually depend on rope-specific knowledge
and assumptions.
For instance, in knot planning from observation (KPO), Morita et al. and later
Takamatsu et al. used knot theory to recognize rope configurations and define
movement primitives in visual observations of humans tying
knots~\cite{Morita_ICRA2003, Takamatsu_TransRob2006}. However, they focused on
recognition and did not apply their work to execution of knot tying.
Saha et al. and Wakamatsu et al. proposed motion planning approaches for knot tying,
using topological representations of rope states (i.e. sequences of rope crossings
and their properties). Wakamatsu et al. defined four basic operations for
transitioning between topological rope states,
while Saha et al. required a model of state-transition functions
as input~\cite{Saha_ExpRobotics2008, Wakamatsu_IJRR2006}.
Robust open loop execution of knot tying has also been explored by Bell~\cite{Bell_PhD2010}.
In contrast to these previous approaches, our proposed appraoch does not
explicitly make use of underlying rope-specific
knowledge or directly model the rope; instead, it infers this knowledge by
robustly applying human-guided demonstrations to new scenes thorugh
\mmql{}-driven demonstration selection and trajectory transfer.

A final area of related work stems from hierarchical reinforcement learning. 
In this setting, a policy, potentially initialized from expert demonstrations, is learned given the ability to experiment in the world.
There is some similarity in our abstract MDP formulation to the options framework for temporal abstraction~\cite{sutton1999between}.
In their framework, they make learning easier by allowing the ability to learn policy that will eventually reach a goal state.
We make use of a similar type of abstraction, but our policies are learned from demonstrations and do not satisfy all the requirements to be an option.

\citet{konidaris2010constructing} use learning from demonstrations to initialize a skill chaining reinforcement learning algorithm.
However, they focus on taking a demonstration and decomposing it into explicit local policies.
We simply make use of the associated trajectory transfer and leverage it to solve tasks beyond the reach of current reinforcement learning approaches.
\citet{Neumann09learningcomplex} propose the idea of motion templates, parametrized local controllers as another way to elicit this behavior in RL.
When they learn the templates, they must find an appropriate policy for setting these parameters, which is intractable in our setting.
