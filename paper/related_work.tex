\section{Related Work}
Related work for our contribution stems from three areas of research:
learning from demonstrations, deformable object manipulation (in particular
knot tying), and hierarchical reinforcement learning.\dhm{We think this is too long, but haven't had time to clip it}

%\subsection{Learning from Demonstrations}
The problem of learning from demonstrations deals with the generalization of expert demonstrations to new scenarios. 
This is a broad area of research with many different approaches and broad applicability~\cite{Argall_2009, Schaal_1999}.
The approach to learning from demonstrations applied here is most closely linked to behavioral cloning, where an agent attempts to learn a function that directly maps states to actions in order to mimic the expert's behavior. 

One of the first successful applications of this strategy is the ALVINN system~\cite{Pomerleau_NIPS1989}, which utilizes a neural network to learn a policy that enables an autonomous car to follow a road.
ALVIN learns a function that maps image input and range finder input to left and right steering commands. 
This research direction has been furthered more recently by \citet{muller2005off}, who use a convolutional network to learn a steering policy for off-road driving.
\citet{Ratliff_Humanoids2007} uses multi-class classification to learn a function that scores actions to predict good foot steps for robot locomotion and good grasps for robot manipulation.
\citet{Ross_2013} propose a method to directly control a Micro UAV from RGB camera input and describe a method to account for differences in the training distribution derived from expert demonstrations and the induced distribution under a learned policy. 
These approaches differ from our proposed method in that they learn in low-dimensional action spaces and directly imitate the expert.
In contrast, \mmql{} copes with high-dimensional state and action spaces, and accounts for temporal constraints from demonstrated behavior.

Miyamoto et al. describe an approach for learning to play Kendama~\cite{Miyamoto_1996} and hit a tennis ball~\cite{Miyamoto_1998} from demonstrated actions. 
Their method is successful at generalizing human trajectories and incorporates sequential information from multiple demonstrations.
However, their approach requires hand tuning of waypoints and does not generalize to new scenes.

The behavioral cloning work most similar to our approach is that of \citet{Isaac_ICML2003}.
They use behavioral cloning to learn to fly an airplane, by making use of an abstract goal-directed layer which sits on top of a low-level PID controller.
This goal-directed learning is similar in spirit to ours, although it makes use of a different formalism.
One limitation of this work is that the low-level control are all PID controllers, which is 
limits the applications it can be applied to.

Similarly to our approach, \citet{Dvijotham_ICML2010} also attempt to directly
learn a value function or Q-function for a MDP, given sample transitions
generated by an optimal control policy. However, they assume either a discrete
state space or a linear dynamics model. In contrast, our method makes no
assumptions about the size of the state space or the dynamics model; it instead
relies on segments of expert demonstrations as a means of navigating the state
space efficiently. \et{Someone who actually understands robotics should probably
  check this statement.}

%\subsection{Deformable Object Manipulation}
Because our approach of \mmql{} makes no assumptions about the state space
or the dynamics of the model, it can be applied towards a variety of tasks
in robotics, including the manipulation of deformable objects.
It is challenging to manipulate deformable objects due to their nonlinearity,
and because the configuration spaces of such objects may be
infinite-dimensional~\cite{Lamiraux_IJRR2001}.

In previous work on manipulation of deformable objects,
Wada et al. perform indirect simultaneous positioning of
textile fabric and sponge blocks by modeling the
the objects coarsely and then applying a control method robust
to discrepancies between the coarse model and the
object~\cite{Wada_ArticMotion2000}. Howard et al. present a
general approach for grasping 3D deformable objects
that does not assume prior knowledge of the object.
They model particle motion of the object using nonlinear partial differential
equations, and train a neural network for determining the minimum force
required for manipulating the object~\cite{Howard_AutRobots2000}.
In contrast, our approach enables manipulation of deformable objects without
the need to directly model the object. In addition, for tasks
where a model of state-transitions is available, our use of
lookahead to augment \mmql{} reduces uncertainty in execution through
simulation of transferred trajectories.

In particular, we demonstrate the effectiveness of our approach for
knot tying, a commonly studied manipulation task in robotics.
Previous approaches to knot tying usually depend on rope-specific knowledge
and assumptions.
For instance, in knot planning from observation (KPO), Morita et al. and later
Takamatsu et al. use knot theory to recognize rope configurations and define
movement primitives from visual observations of humans tying
knots~\cite{Morita_ICRA2003, Takamatsu_TransRob2006}. However, they focus on
recognition and did not apply their work to execution of knot tying. Matsuno et al.
also uses a topological model of the rope and knot theory to recognize knots with
stereo vision input~\cite{Matsuno_RSJ2006}.
Saha et al. and Wakamatsu et al. propose motion planning approaches for knot tying,
using topological representations of rope states. Wakamatsu et al. define four basic operations for
transitioning between topological rope states,
while Saha et al.'s approach requires a model of state-transition functions
as input~\cite{Saha_ExpRobotics2008, Wakamatsu_IJRR2006}. Moll et al.'s approach
to motion planning is based on restricting the planner to minimal-energy
configurations of the deformable linear object, e.g. a wire or rope~\cite{Moll_IEEERobot2006}.
Robust open loop execution of knot tying has also been explored by Bell~\cite{Bell_PhD2010}.
In contrast to these previous approaches, our proposed approach does not
explicitly make use of underlying rope-specific
knowledge or directly model the rope; instead, it infers this knowledge by
robustly applying human-guided demonstrations to new scenes through
\mmql{}-driven demonstration selection and trajectory transfer.



A final area of related work stems from hierarchical reinforcement learning. 
In this setting, a policy is learned given the ability to experiment in the world.
This policy is potentially initialized from expert demonstrations.
There is some similarity in our abstract MDP formulation to the options 
framework for temporal abstraction~\cite{sutton1999between}.
The options framework simplifies learning by enabling temporal abstraction through
policies that are gauranteed to reach a target state. 
This lets learning agent abstract away low-level details of a task and focus on 
high level interactions. 
We make use of a similar type of abstraction, but our policies are learned from 
demonstrations and do not satisfy all the requirements to be an option.

\citet{konidaris2010constructing} use learning from demonstrations to initialize a skill chaining reinforcement learning algorithm.
However, they focus on taking a demonstration and decomposing it into explicit local policies.
We simply make use of the associated trajectory transfer and leverage it to solve tasks beyond the reach of current reinforcement learning approaches.
\citet{Neumann09learningcomplex} propose the idea of motion templates using parametrized local controllers as another way to elicit this behavior in RL.
When they learn the templates, they must find an appropriate policy for setting these parameters, which is intractable in our setting.
