\section{Related Work}
Related work for our contribution stems from three areas of research: deformable object manipulation (in particular knot-tieing), learning from demonstrations, \dhm{and a third thing}
\subsection{Deformable Object Manipulation}
Our approach can be applied towards a variety of tasks in robotics,
including the manipulation of deformable objects.
In particular, we demonstrate the effectiveness of our approach for
knot tying, a commonly studied manipulation task in robotics.
Previous approaches to knot tying usually depend on rope-specific knowledge
and assumptions.
For instance, in knot planning from observation (KPO), knot theory is used
to recognize rope configurations and define movement primitives in visual
observations of humans tying knots \cite{Morita_ICRA2003, Takamatsu_TransRob2006}.
Existing motion planning approaches for knot tying use topological
representations of rope states (i.e. sequences of rope crossings and their
properties) and define a model for transitioning between topological states
\cite{Saha_ExpRobotics2008, Wakamatsu_IJRR2006}.
Robust open loop execution of knot tying has also been explored \cite{Bell_PhD2010}.

\subsection{Learing from Demonstrations}
The problem of learning from demonstrations deals with the generalization of expert demonstrations to new scenarios. 
This is a broad area of research with many different approaches and broad applicabilty.\dhm{Cite Veloso survey and Schaal paper}
The approach to learning from demonstrations applied here is most closely linked to behavioral cloning; where an agent atempts to learn a function that directly maps states to actions so as to mimic the experts behavior. 

One of the first successfull applications of this strategy was the ALVINN system, which utilized a neural network to learning a policy such that an autonomous car could follow a road\dhm{CITE}.
In this a function is learned that can map image input and range finder input to left and right steering commands. 
This research direction has been furthered recently by Lecun et al. \dhm{CITE} who use a convolutional network to learn a steering policy for off-road driving.
Bagnell et al. propose a method to directly control a Micro UAV from RGB camera input and describe a method to account for differences in the training distribution under expert demonstration and the induced distribution under a learned policy\dhm{CITE}. 
These approaches differ from the proposed method in that they do learning for low-dimensional action spaces and directly imitate the expert.
In contrast, MMQL copes with high dimensional state and action spaces as well as accounting for temporal constraints from demonstrated behavior.

Miyamoto et al. \dhm{CITE} describe an approach for learning to play Kendama and hit a tennis ball from demonstrated actions. 
This method was successful at generalizing a human trajectory to a robot and incorporates the sequential combination of multiple demonstrations.
However, their approach requires a hand tuning of waypoints and does not generalize to new scenes.
