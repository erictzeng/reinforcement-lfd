\section{Related Work}

%%PA: it says currently that behavioral cloning is closest, but then
%%it comes across as if only the state-action mapping is learned,
%%whereas we have both the state-action mapping constraints and the
%%value function / Bellman equation constraints in our learning
%%formulation

Related work for our contribution stems from three areas of research:
learning from demonstrations, deformable object manipulation (in particular
knot tying), and hierarchical reinforcement learning.

%\subsection{Learning from Demonstrations}
The problem of learning from demonstrations (LfD) deals with the generalization of expert demonstrations to 
new scenarios~\cite{Argall_2009, Schaal_1999}. Behavioral cloning is an approach to LfD that 
directly learns a policy to mimic an expert's behavior.

One of the first successful applications of this strategy is the ALVINN system~\cite{Pomerleau_NIPS1989}, which utilizes a 
neural network to learn a steering policy that enables an autonomous car to follow a road.
\citet{muller2005off} use a convolutional network to learn a steering policy for off-road driving.
\citet{Ratliff_Humanoids2007} uses multi-class classification to learn a function that scores actions 
to predict good foot steps for robot locomotion and good grasps for robot manipulation.
\citet{Ross_2013} propose a method to directly control a Micro UAV from RGB camera input.

Miyamoto et al. describe an approach for learning to play Kendama~\cite{Miyamoto_1996} and hit a 
tennis ball~\cite{Miyamoto_1998} from demonstrated actions. 
Their method is successful at generalizing human trajectories and incorporates sequential information 
from multiple demonstrations.
However, this approach requires hand tuning of waypoints and does not generalize to new scenes.

\citet{Isaac_ICML2003} use behavioral cloning to learn to fly an airplane, by making use of an abstract, 
goal-directed, layer which sits on top of a low-level PID controller.
This goal-directed learning is similar in spirit to ours, although it makes use of a different formalism
and uses simpler low level controllers.

Calinon et al. learn a mixture of Gaussians to represent the joint trajectory of the robot and environment
state across multiple demonstrations, and infer the trajectory for a new
environment state by conditioning on that state~\cite{Calinon_SMC2007, Calinon_HUM2009}. Their approach
assumes access to a feature representation of the environment, so it cannot be applied to tasks in
environments without fixed feature representations --- such as our application of knot tying.

\citet{Dvijotham_ICML2010} directly
learn a value function or Q-function for a MDP, given sample transitions
from an optimal control policy. They learn the expert's 
reward function and are limited to models with tractable discrete representations 
or linear dynamics models.
In contrast, our approach fixes a cost function 
and use learning to account for the prohibitive size and
complex dynamics.

%\subsection{Deformable Object Manipulation}

%%PA: someone might challenge the fact that we don't make any
%%assumptions; it's true we don't make any explicit assumptions for
%%the max-margin formulation, but there is some assumptoin that the
%%warping is meaningful relative to the dynamics, and then when doing
%%lookahead we are actually simulating ...
 
Because our approach of \mmql{} makes limited assumptions about the state space
and dynamics of the model, it can be applied towards a variety of tasks
in robotics, including the manipulation of deformable objects.
It is challenging to manipulate deformable objects due to their nonlinearity
and because the configuration spaces of such objects may be
infinite-dimensional~\cite{Lamiraux_IJRR2001}.

In previous work, Wada et al. model textile fabric and sponge blocks
coarsely and then apply a control method that is robust
to discrepancies between the coarse model and the
object~\cite{Wada_ArticMotion2000}. Howard et al. present a more
general approach for grasping 3D deformable objects
that does not assume prior knowledge of the object.
They model particle motion of the object using nonlinear partial differential
equations, and train a neural network for determining the minimum force
required for manipulating the object~\cite{Howard_AutRobots2000}.
In contrast, our approach enables manipulation of deformable objects
without directly modeling the object, and makes no assumptions beyond
those of trajectory transfer. When a model of state-transitions
is available, our use of
lookahead to augment \mmql{} reduces uncertainty in execution through
simulation of transferred trajectories.

We demonstrate the effectiveness of our approach for
knot tying, a commonly studied deformable object manipulation task in robotics.
Previous approaches to knot tying usually depend on rope-specific knowledge
and assumptions.
For instance, in knot planning from observation (KPO), knot theory is used
to recognize rope configurations and define
movement primitives from visual observations of humans tying
knots~\cite{Morita_ICRA2003, Takamatsu_TransRob2006}.
Existing motion planning approaches for knot tying use topological
representations of rope states (i.e. sequences of rope crossings and their
properties) and define a model for transitioning between topological
states~\cite{Moll_IEEERobot2006, Saha_ExpRobotics2008, Wakamatsu_IJRR2006}.
Robust open loop execution of knot tying has also been explored~\cite{Bell_PhD2010}.
In contrast to these previous approaches, our proposed approach does not
explicitly make use of underlying rope-specific
knowledge or directly model the rope; instead, it infers this knowledge by
robustly applying human-guided demonstrations to new scenes through
\mmql{}-driven demonstration selection and trajectory transfer.

A final area of related work stems from hierarchical reinforcement learning. 
In this setting, a policy is learned given the ability to experiment in the world.
This policy is potentially initialized from expert demonstrations.
There is some similarity in our abstract MDP formulation to the options 
framework for temporal abstraction~\cite{sutton1999between}.
The options framework simplifies learning by enabling temporal abstraction through
policies that are gauranteed to reach a target state. 
This lets learning agent abstract away low-level details of a task and focus on 
high level interactions. 
We make use of a similar type of abstraction, but our policies are learned from 
demonstrations and do not satisfy all the requirements to be an option.

\citet{konidaris2010constructing} use learning from demonstrations to initialize a skill chaining reinforcement learning algorithm.
However, they focus on taking a demonstration and decomposing it into explicit local policies.
We simply make use of the associated trajectory transfer and leverage it to solve tasks beyond the reach of current reinforcement learning approaches.
Neumann et al. and Stulp et al. both explore using optimizing motion primitive parameters as another way to elicit this behavior in RL ~\cite{Neumann09learningcomplex, Stulp_ROB2012}.
When they learn the primitives, they must find an appropriate policy for setting these parameters, which is intractable in our setting.
