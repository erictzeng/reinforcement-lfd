\section{Problem Description and Formulation}
\label{sec:formulation}
%\input{variables.tex}

In this section, we will formulate the Max Margin Q-Learning (\mmql{}) approach and
specify our assumptions regarding its applicability.

\subsection{Motivation and Overview}
% shortcomings of NN approach (motivation)
To facilitate discussion, consider a simple trajectory transfer
problem with states $s_i$ and two expert demonstrations, $A, B$. Let
$rc(s_0, A)$ be the registration cost associated with transferring $A$
to $s_0$. The nearest neighbor selection rule from
\citet{Schulmanetal_ISRR2013} proposes to select $\argmax_{D \in \{A,
  B\}} rc(s_0, D)$ in state $s_0$.

A drawback of this approach is that all demonstrations are treated
identically but some may transfer more readily.  $A$ may transfer
successfully if its registration cost is below 10, while $B$ will only
transfer successfully if its registration cost is below 1.
Additionally, this measure does not incorporate cost-to-go. If $A$ and
$B$ have a comparable chance of successfully transferring, but $B$
makes more progress towards the goal, one should transfer
$B$. Finally, while registration cost provides a relative ranking of
options for a given state, it is incorrect as a comparison between
different states. That is, it is reasonable to compare $rc(s_1, A)$
and $rc(s_1, B)$, but not to compare $rc(s_1, A)$ and $rc(s_2,
B)$. This prevents explicit search over options (e.g. simulating
transfer of different trajectories and comparing the results). As an
example, the minimum registration cost increases over the course of a
successful overhand-knot tie, so minimizing registration cost over a
search horizon is easily at odds with making progress towards the
goal.

We approach this problem by formulating our trajectory transfer
problem as an abstract MDP.  In this MDP, the state space is the same,
but the actions correspond to selecting a trajectory and transferring
it to the current state. The state-action pair $(s, A)$ corresponds to
transferring demonstration $A$ to $s$ and executing the result. This
MDP has a reward of 0 for tying a knot and a reward of -1 otherwise
(to elicit goal-directed behavior). The Q-function for this MDP meets
the criteria above. It gives a ranking of different actions given a
state that incorporates cost-to-go. Furthermore, it is an absolute
measure of quality and can be compared across different states.

One could apply reinforcement learning to learn this
Q-function. However, efficient exploration presents a significant
challenge, as there is still a large action space and many actions
will simply cause failure. Thus, we propose \mmql{} as a way to learn
a Q-function from sequences of successful trajectory transfers. We
take as input sequences of state-action pairs that successfully tie a
knot, where each state is labelled with the demonstration (action for
our abstract MDP) that was transferred to it.

From these sequences, there are two types of constraints on the
Q-function. First, it should rank the demonstrated actions above other
options. This is naturally represented as max-margin
constraints. Second, Q-values of subsequent states should be higher
than those that precede them, because we are making progress towards
the goal. This is naturally represented through the Bellman equations
from the ALP formulations of approximate value iteration. \mmql{}
formalizes these into a single optimization and finds an approximate
Q-function that satisfies our desiderata.


% what does combining them entail and why do we think it will work

%%PA: throughout the paper there are quite a few abbreviations used
%%that we are introducing in this paper ; are they really necessary,
%%or could we just spell things out (which tends to be clearer)
%%PA: either way, never use an abbreviation without introducing it;
%%and it's tricky if it's been introduced a long time ago; let's not
%%burden our reviewers ...

%%PA: could it make sense to build it up with first presenting
%%max-margin direct policy learning

%%PA: and then say, well, we are not using some information (about
%%Bellman updates for sequential states in the demonstration) and we
%%can't do lookahead planning with this; so now let's take a look at
%%how to add that to our formulation;  

\subsection{Demonstration Set and Labeled Examples}

The required inputs of our pipeline are a library \demoset{}
of expert demonstrations and a set \labelset{} of labeled sequences of
task executions.
Each demonstration \demovar{} $\in$ \demoset{} corresponds to a step
of the given robotic task. Ideally, the segments in \demoset{} contain
at least several demonstrations of each step of the task.

A demonstration \demovar{} is composed of (\demosub{pc}, \demosub{traj}),
where \demosub{pc} is a point cloud representation of the underlying
starting state of the demonstration, and \demosub{traj} is
the trajectory executed by the expert in that demonstration.

Each \labelsub{j} $\in$ \labelset{}
corresponds to a labeled sequence of
task executions of length \nsub{j}:
\begin{equation*}
\labelsub{j} = [\sapairsubsup{1}{(j)},
\sapairsubsup{2}{(j)}, \ldots, \sapairsubsup{\nsub{j}}{(j)}].
\end{equation*}
\statesubsup{i}{(j)} is a state of the task (which is represented as
the point cloud associated with that state). \actionsubsup{i}{(j)} is
the demonstration we transferred to that state. Each labeled sequence
\labelsub{j} corresponds to a single complete task execution and is ordered
chronologically: \statesubsup{i}{(j)} is the result of taking action
\actionsubsup{i-1}{(j)} in state \statesubsup{i-1}{(j)}, and
\statesubsup{\nsub{j}}{(j)} is a goal state.

%%PA: How about in subsection1 presenting the problem formulation, in which the
%%demonstration set is described as well as the labeled examples; then
%%in subsection2 present max-margin direct policy learning; then in
%%subsection3 present max-margin Q-learning, which also explains the
%%MDP formalization; and then in subsection4 Elimination of Manual
%%Labeling;  I think this way the subsection titles will reflect the
%%structure; the reader should be able to follow easily through
%%subsection2 and also have an easy time understanding the motivation
%%for subsection3 (maybe be lost in subsection3, though hopefully not,
%%but either way, as long as they understand from the first few
%%motivating sentences of subsection3 what it's getting it, they get
%%the idea anyway even if they don't get the math); 

\subsection{Markov Decision Process Formulation}
\label{subsec:form_mdp}

We frame the problem of demonstration selection as a Markov Decision
Process (MDP), which enables us to apply Q-function approximation to derive a
policy.

We use the set of demonstrations \demoset{} to define a set of abstract actions 
\actionset{}. Each abstract action $\actionvar \in \actionset$ represents a 
policy $\pi_a$, that transfers the corresponding demonstration and executes 
the result. Each action $\actionvar \in \actionset$ has a corresponding demonstration $\demovar
\in \demoset$, we will refer to actions and demonstrations
interchangeably. $\policysub{a}{\statevar} =
\policysub{d}{\statevar}$ is most precisely defined as the result of 
trajectory transfer from $\demovar$ to $\statevar$. However, since our
transition model is deterministic, applying this trajectory to $\statevar$ will
always produce the same successor $s'$. We overload
$\pi_{d}$ to be the successor function for transferring $d_{traj}$ to a state $s$ 
and executing it,and say that $\policysub{d}{\statevar} = s'$.

Thus, the MDP for an \mmql{} problem is defined as
$\langle\stateset,\goalset,\actionset,T,R\rangle$, where

\begin{equation}
\begin{aligned}
\stateset{} &=  \text{all underlying states of the task} \\
\goalset{} &=  \text{all goal states of the task} \\
\actionset{} &= \{\policysub{\demovar{}}{\statevar{}}\ \mid \text{ } \demovar{} \in \demoset{}\} \\
\transitionfn{}(\statevar{}, \actionvar{}, \nextstatevar{}) &=
    \begin{cases}
    1[\policysub{\actionvar{}}{\statevar{}} = \nextstatevar{}] &\text{ if } \statevar{} \not \in \goalset{} \\
    1[\statevar{} = \statevar{}'] &\text{ if } \statevar{} \in \goalset{}
    \end{cases}\\
\rewardfn{}(\statevar{}) &= -1 {[ \statevar{} \not \in \goalset{} ]}.
\end{aligned}
\label{eq:mdp}
\end{equation}

In the above specification, \goalset{} denotes the goal set and contains all
states that satisfy the predefined goal. In the knot-tying example,
\goalset{} denotes the set of tied knots. 

\subsection{Max-Margin Q-Learning Formulation}
Our goal is to use the labeled sequences \labelset{} in order to learn an
approximate Q-function that determines, for a given state, which actions
rank higher than others.
Recall that actions in \actionset{} have a one-to-one correspondence with the
expert-guided demonstrations in \demoset{}, and are defined by executing the
transferred trajectory from the corresponding demonstration to the given state.

The functions we learn are linear combinations of features on states and actions.
For a given state \statevar{} and action
\actionvar{} the corresponding feature vector is \features{\statevar{}}{\actionvar{}}
(described in Section~\ref{sec:features}). Thus our approximate Q-function is defined as
\begin{equation}
\approxq(\statevar{}, \actionvar{}) = \weightszero{} + \weightst{} \features{\statevar{}}{\actionvar{}}.
\end{equation}

An initial approach is to directly apply max margin behavior cloning. 
This corresponds to computing the weights, $w$, as the solution to the following  convex optimization problem: 
\begin{align}
& \underset{w, \xi, \nu}{\min}  & & ||\weights{}||^2 + \marginslackc{} \sum_{j=1}^{\labelsetsize{}} \sum_{i=1}^{\nsub{j}} \marginslacksubsup{i}{(j)}\notag \\
& \text{s.t.} & &\weightst{} \features{\statesubsup{i}{(j)}}{\actionsubsup{i}{(j)}} \geq \weightst{} \phi(\statesubsup{i}{(j)}, \actionvar{}') + \marginvar{}(\statesubsup{i}{(j)}, \actionsub{i}, \actionvar{}') - \marginslacksubsup{i}{(j)} \notag\\
    &&&\hspace{1.9cm}\forall j = 1, \ldots, \labelsetsize{}; \text{ } \forall i = 1, \ldots, \nsub{j}; \notag\\
    &&&\hspace{1.9cm}\forall \actionvar{}' \in \actionset{}\setminus \actionvar{}_i  \label{eq:margin_constr}\\
&    & & \marginslacksubsup{i}{(j)} \geq 0 \hspace{0.7cm} \forall j = 1, \ldots, \labelsetsize{}; \text{ } \forall i = 1, \ldots, \nsub{j} \label{eq:margin_slacks}
\end{align}

The constraints in Equation~\ref{eq:margin_constr}
enforce the max-margin requirements that the
labeled action \actionsubsup{i}{(j)} associated with state \statesubsup{i}{(j)}
has a higher (approximate) Q-value than the other actions for that
state. The slack variables \marginslacksubsup{i}{(j)} relax these constraints;
the degree of relaxation is controlled by the regularization parameter \marginslackc{}.


To account for situations where several demonstrations will generalize well, we use a structured margin.
We structure our margin with the similarity metric
\marginvar{}(\statesubsup{i}{(j)}, \actionsub{i}, \actionvar{}') that computes
the difference between the trajectories of the actions after being warped onto
the state \statesubsup{i}{(j)}. More precisely, we first apply trajectory
transfer on both actions \actionsub{i} and \actionvar{}' to transfer
each trajectory to the state \statesubsup{i}{(j)}. Then, we use dynamic
time warping of the trajectories for robust comparison of trajectories that
vary in time and speed~\cite{Sakoe_IEEE1978, Vakanski_2012}.

The solution to this problem will determine a policy which aproximates the expert's policy.
A standard approach to improving a policy defined by optimizing a heuristic measure action quality
is to perform search and act to maximize action rankings over a horizon. 
However, the rankings from the behavior cloning optimization are state dependent and 
we have no way of comparing actions from different states. Because our data
contains full traces of expert-guided executions, we can incorporate a version
of the approximate linear program for an MDP into our formulation. This is formalized as the 
following optimization:

\begin{align}
& \underset{w, \xi, \nu}{\min}  & & ||\weights{}||^2 - \frac{\bellmanc{}}{\labelsatotal{}} \sum_{j=1}^{\labelsetsize{}} 
  \sum_{i=1}^{\nsub{j}} (\weightszero{} + \weightst{}\features{\statesubsup{i}{(j)}}{\actionsubsup{i}{(j)}})\\
&    & & + \marginslackc{} \sum_{j=1}^{\labelsetsize{}} \sum_{i=1}^{\nsub{j}} 
            \marginslacksubsup{i}{(j)}+ \bellmanslackc{} \sum_{j=1}^{\labelsetsize{}} 
            \bellmanslacksup{(j)} \\
& \text{s.t.} & &\weightst{} \features{\statesubsup{i}{(j)}}{\actionsubsup{i}{(j)}} \geq \weightst{} \phi(\statesubsup{i}{(j)}, \actionvar{}') + \marginvar{}(\statesubsup{i}{(j)}, \actionsub{i}, \actionvar{}') - \marginslacksubsup{i}{(j)} \notag\\
    &&&\hspace{1.9cm}\forall j = 1, \ldots, \labelsetsize{}; \text{ } \forall i = 1, \ldots, \nsub{j}; \notag\\
    &&&\hspace{1.9cm}\forall \actionvar{}' \in \actionset{}\setminus \actionvar{}_i  \notag\\
&    & & \marginslacksubsup{i}{(j)} \geq 0 \hspace{0.7cm} \forall j = 1, \ldots, \labelsetsize{}; \text{ } \forall i = 1, \ldots, \nsub{j} \notag\\
&    & & \weightst{}\features{\statesubsup{i}{(j)}}{\actionsubsup{i}{(j)}} \leq \rewardfn{}(\statesubsup{i}{(j)}) + \weightst{}\features{\statesubsup{i+1}{(j)}}{\actionsubsup{i+1}{(j)}} + \bellmanslacksup{(j)} \notag \\
    &&&\hspace{1.9cm} \forall j = 1, \ldots, \labelsetsize{}; \text{ } \forall i = 1, \ldots, \nsub{j} - 2 \label{eq:bellman_constr}\\
&    & & \weightst{}\features{\statesubsup{\nsub{j}-1}{(j)}}{\actionsubsup{\nsub{j}-1}{(j)}} + \weightszero \leq \rewardfn{}(\statesubsup{\nsub{j}-1}{(j)}) + \bellmanslacksup{(j)} \notag \\
    &&&\hspace{1.9cm} \forall j = 1, \ldots, \labelsetsize{} \label{eq:bellman_goal_constr}\\
&    & & \bellmanslacksup{(j)} \geq 0 \hspace{0.7cm} \forall j = 1, \ldots, \labelsetsize{} \label{eq:bellman_slack}
\end{align}

where \labelsatotal{} is equal to the total number of
(\statevar{}, \actionvar{}) pairs in \labelset{}, so \labelsatotal{} = 
$\sum_{j=1}^{\labelsetsize{}} \sum_{i=1}^{\nsub{j}} 1$.


The Bellman constraints in Equation~\ref{eq:bellman_constr} and~\ref{eq:bellman_goal_constr}
are key for learning a valid Q-function approximation \approxq. These constraints specify that
the values of the states in the labeled example task executions increase as they
approach the goal. In the objective, we effectively maximize the sum of the
approximate Q-values of the non-goal state-action pairs in the labeled examples,
thus driving the Bellman constraints to equality. By enforcing the value of goal states
to be zero, we arrive at the constraints in Equation~\ref{eq:bellman_goal_constr} for
states immediately prior to the goal in the labeled sequences.
The \weightszero{} term allows for the \approxq{} to be affine; it exists on
both sides of the inequality of Equation~\ref{eq:bellman_constr} and thus cancels out.
We can also ignore \weightszero{} when using \approxq{} to define a policy. The slack variables
\bellmanslacksup{(j)} relax these constraints, with \bellmanslackc{} controlling
the degree of relaxation.
\rewardfn{}(\statevar{}) is as defined in Equation~\ref{eq:mdp}:
-1 for non-goal states and 0 for goal states.

\subsection{Elimination of Manual Labeling}
\label{subsec:lool}
One major limitation of the formulation we have outlined is its dependence on
the set of labeled sequences \labelset{}. We now propose an alternative method
of bootstrapping the set of expert demonstrations \demoset{} to generate an
unsupervised set of labeled sequences, $\labelset{}_u$, and we refer to this
method as leave-one-out labeling.

For each demonstration $\demovar{} \in \demoset{}$, we generate a corresponding
$\labelvar \in \labelset_u$ as follows. For every other demonstration
$\demovar{}' \neq \demovar{} \in \demoset{}$, we simply compute the TPS
registration cost between $\policysub{\demovar{}'}{d_{pc}}$ and
$\policysub{\demovar{}}{\demovar{}_{pc}}$. We choose the $\demovar{}^*$ with the
lowest registration cost, and add $(\demovar{}_{pc}, \demovar{}^*_{traj})$ to
$\labelset{}_u$ as an expert demonstration. Intuitively, this procedure looks to
discover the expert demonstration $d'$ that, when applied to the start state of
$d$, most closely mimics the optimal choice provided by the expert.

This method of labeling requires slight modifications to our max-margin
formulation. Since the expert's trajectory is assumed to be optimal for its
corresponding state, we exclude each expert demonstration from the max margin
constraints its corresponding unsupervised example generates. Additionally, note
that this unsupervised method of generating labeled sequences does not include a
temporal component. Thus, when forming the optimization problem with
$\labelset{}_u$, we must omit Bellman constraints.
