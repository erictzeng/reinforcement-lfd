\section{Problem Description and Formulation}
\label{sec:formulation}
%\input{variables.tex}

In this section, we will formulate our approach to doing {\sc mmpl}, and
specify our assumptions regarding the problems it is applied to.

\subsection{Demonstration Set}

The required inputs of our pipeline are a set \demoset{}
of expert demonstrations and a set \labelset{} of labeled sequences of
task executions (Section~\ref{subsec:labeledex}).
Each \demovar{} $\in$ \demoset{} corresponds to a complete expert-guided
demonstration of the given robotic task or a movement primitive of a complete
demonstration --- our {\sc mmpl} formulation assumes the latter,
since it is often necessary for complex, multi-step tasks.
In our application of knot tying, we split each expert demonstration of
tying a knot into three or four movement primitives. The movement primitives
also include demonstration segments that attempt to
recover from failure states \cite{Schulmanetal_ISRR2013}.
Ideally, the segments in \demoset{} contain at least several demonstrations
of each movement primitive in the task.

A demonstration \demovar{} is composed of (\demosub{pc}, \demosub{traj}),
where \demosub{pc} is a point cloud representation of the underlying
starting state of the demonstration, and \demosub{traj} stores
the trajectory executed by the expert in that demonstration. The trajectory
space \trajset{} spans the set of all possible trajectories.


\subsection{Markov Decision Process Formulation}
Selecting from multiple demonstrations can be framed as a Markov Decision
Process (MDP), which enables us to apply Q-function approximation to derive a
policy.

In order to make planning in this space tractable, we use the set of
demonstrations \demoset{} to define a set of abstract actions \actionset{}. Each
action $\actionvar \in \actionset$ denotes the choice to use a corresponding
policy $\pi_d$, where we apply $\pi_d$ to some state $s$ by transferring
$d_{traj}$ to $s$ via TPS warping, then executing the resulting trajectory.

Since each action $\actionvar \in \actionset$ has a corresponding demo $\demovar
\in \demoset$, from here onward we refer to actions and demos
interchangeably. Additionally, $\policysub{d}{\statevar} =
\policysub{a}{\statevar}$ is most precisely defined as the trajectory that is
found via trajectory transfer from $\demovar$ to $\statevar$. However, since our
transition model is deterministic, applying this trajectory to $\statevar$ will
always produce the same successor $s'$. Thus, for ease of notation we will use
$\pi_{d}$ as the successor function for transferring $d_{traj}$ to a state $s$,
and say that $\policysub{d}{\statevar} = s'$.

Thus, the MDP for an \mmql{} problem is defined as
$\langle\stateset,\goalset,\actionset,T,R,H\rangle$, where

\begin{equation}
\begin{aligned}
\stateset{} &=  \text{all underlying states of the task} \\
\goalset{} &=  \text{all goal states of the task} \\
\actionset{} &= \{\policysub{\demovar{}}{\statevar{}}\ \mid \text{ } \demovar{} \in \demoset{}\} \\
\transitionfn{}(\statevar{}, \actionvar{}, \nextstatevar{}) &=
    \begin{cases}
    1[\policysub{\actionvar{}}{\statevar{}} = \nextstatevar{}] &\text{ if } \statevar{} \not \in \goalset{} \\
    1[\statevar{} = \statevar{}'] &\text{ if } \statevar{} \in \goalset{}
    \end{cases}\\
\rewardfn{}(\statevar{}) &= -1 {[ \statevar{} \not \in \goalset{} ]}. \\
H &= \text{finite horizon}
\end{aligned}
\end{equation}

In the above specification, \goalset{} denotes the goal set and contains all
states that satisfy the predefined goal. For example, in the knot-tying example,
\goalset{} denotes the set of tied knots.  Our MDP's reward function is then
simply $-1$ for all non-goal states and $0$ for goal states.

\subsection{Labeled Examples}
\label{subsec:labeledex}

As mentioned, one of the required inputs to our pipeline is a set \labelset{}
of labeled sequences of task executions. Each \labelsub{j} $\in$ \labelset{}
corresponds to a labeled sequence of
task executions of length \nsub{j}: \labelsub{j} = [\sapairsub{1},
\sapairsub{2}, \ldots, \sapairsub{\nsub{j}}]. Each labeled sequence corresponds
to a single complete task execution, ordered in the sequence of states and
actions taken. More formally, it is required that for each trajectory
\labelsub{j} $\in$ \labelset{}, for $1 \leq i \leq$ \nsub{j},
\begin{align*}
\transitionfn(\statesubsup{i}{(j)}, \actionsubsup{i}{(j)}) &= \statesubsup{i+1}{(j)} \text{ for } i < \nsub{j} \\
\text{and } T(\statesubsup{\nsub{j}}{(j)}, \actionsubsup{\nsub{j}}{(j)}) &\in \goalset{}
\end{align*}

\subsection{Max-Margin Q-Learning Formulation}
Our goal is to use the labeled sequences \labelset{} in order to learn an
approximate Q-function that determines, for a given state, which actions
rank higher than others.
Recall that actions in \actionset{} have a one-to-one correspondence with the
expert-guided demonstrations in \demoset{}, and are defined by executing the
transferred trajectory from the corresponding demonstration to the given state.

We approximate the Q-function for a given state \statevar{} and action
\actionvar{} as a linear combination of features \features{\statevar{}}{\actionvar{}}
(Section~\ref{sec:features}):
\begin{equation}
\approxq(\statevar{}, \actionvar{}) = \weightst{} \features{\statevar{}}{\actionvar{}}
\end{equation}

To calculate the weights, we construct an optimization problem within the
max-margin framework that incorporates max-margin constraints with a structured
margin, as well as Bellman constraints for Q-learning. The weights of \approxq{}
are then the solution of this optimization problem, which is as follows:
\begin{align}
& \underset{w, \xi, \nu}{\text{min}}  & & ||\weights{}||^2 + \marginslackc{} \sum_{j=1}^{\labelsetsize{}} \sum_{i=1}^{\nsub{j}} \marginslacksubsup{i}{(j)}
                                                      + \bellmanslackc{} \sum_{j=1}^{\labelsetsize{}} \bellmanslacksup{(j)} \notag \\
&    & & - \frac{\bellmanc{}}{\labelsatotal{}} \weightszero{} - \frac{\bellmanc{}}{\labelsatotal{}} \sum_{j=1}^{\labelsetsize{}} \sum_{i=1}^{\nsub{j}} \weightst{}\features{\statesubsup{i}{(j)}}{\actionsubsup{i}{(j)}}\\
& \text{s.t.} & &\weightst{} \features{\statesubsup{i}{(j)}}{\actionsubsup{i}{(j)}} \geq \weightst{} \phi(\statesubsup{i}{(j)}, \actionvar{}') + \marginvar{}(\statesubsup{i}{(j)}, \actionsub{i}, \actionvar{}') - \marginslacksubsup{i}{(j)} \notag\\
    &&&\hspace{1.9cm}\forall j = 1, \ldots, \labelsetsize{}; \text{ } \forall i = 1, \ldots, \nsub{j}; \notag\\
    &&&\hspace{1.9cm}\forall \actionvar{}' \in \actionset{}\setminus \actionvar{}_i  \label{eq:margin_constr}\\
&    & & \marginslacksubsup{i}{(j)} \geq 0 \hspace{0.7cm} \forall j = 1, \ldots, \labelsetsize{}; \text{ } \forall i = 1, \ldots, \nsub{j} \label{eq:margin_slacks}\\
&    & & \weightst{}\features{\statesubsup{i}{(j)}}{\actionsubsup{i}{(j)}} \leq \rewardfn{}(\statesubsup{i}{(j)}) + \weightst{}\features{\statesubsup{i+1}{(j)}}{\actionsubsup{i+1}{(j)}} + \bellmanslacksup{(j)} \notag \\
    &&&\hspace{1.9cm} \forall j = 1, \ldots, \labelsetsize{}; \text{ } \forall i = 1, \ldots, \nsub{j} - 2 \label{eq:bellman_constr}\\
&    & & \weightst{}\features{\statesubsup{\nsub{j}-1}{(j)}}{\actionsubsup{\nsub{j}-1}{(j)}} + \weightszero \leq \rewardfn{}(\statesubsup{\nsub{j}-1}{(j)}) + \bellmanslacksup{(j)} \notag \\
    &&&\hspace{1.9cm} \forall j = 1, \ldots, \labelsetsize{} \label{eq:bellman_goal_constr}\\
&    & & \bellmanslacksup{(j)} \geq 0 \hspace{0.7cm} \forall j = 1, \ldots, \labelsetsize{} \label{eq:bellman_slack}
\end{align}

where \labelsatotal{} is equal to the total number of
(\statevar{}, \actionvar{}) pairs in \labelset{}, so \labelsatotal{} = 
$\sum_{j=1}^{\labelsetsize{}} \sum_{i=1}^{\nsub{j}} 1$.

In the optimization problem, the constraints in Equation~\ref{eq:margin_constr}
enforce the max-margin requirements that the
labeled example's action \actionsubsup{i}{(j)} for state \statesubsup{i}{(j)}
has a higher (approximate) Q-value than the other actions for that
state. The slack variables \marginslacksubsup{i}{(j)} relax these constraints;
the degree of relaxation is controlled by the regularization parameter \marginslackc{}.
We build a structured margin, that allows actions similar to the
action of the labeled example to have a higher approximate Q-value.
We structure our margin with a similarity metric
\marginvar{}(\statesubsup{i}{(j)}, \actionsub{i}, \actionvar{}') that computes
the difference between the trajectories of the actions after being warped onto
the state \statesubsup{i}{(j)}. More precisely, we first apply trajectory
transfer on both actions \actionsub{i} and \actionvar{}' to transfer
each trajectory to the state \statesubsup{i}{(j)}. Then, we use dynamic
time warping of the trajectories for robust comparison of trajectories that
vary in time and speed. \sh{Add citation here for DTW}

The Bellman constraints in Equation~\ref{eq:bellman_constr} and~\ref{eq:bellman_goal_constr}
are key for learning a valid Q-function approximation \approxq. These constraints specify that
the values of the states in the labeled example task executions increase as they
approach the goal. In the objective, we effectively maximize the sum of the
approximate Q-values of the non-goal state-action pairs in the labeled examples,
thus driving the Bellman constraints to equality. By enforcing the value of goal states
to be zero, we arrive at the constraints in Equation~\ref{eq:bellman_goal_constr} for
states immediately prior to the goal in the labeled sequences.
The \weightszero{} term allows for the \approxq{} to be affine; it exists on
both sides of the inequality of Equation~\ref{eq:bellman_constr} and thus cancels out.
We can also ignore \weightszero{} when using \approxq{} to define a policy. The slack variables
\bellmanslacksup{(j)} relax these constraints, with \bellmanslackc{} controlling
the degree of relaxation.
\rewardfn{}(\statevar{}) is as defined in Equation~\ref{eq:mdp},
-1 for non-goal states and 0 for goal states.

\subsection{Leave-One-Out Labeling}
\label{subsec:lool}
One major limitation of the formulation we have outlined is its dependence on
the set of labeled sequences \labelset{}. We now propose an alternative method
of bootstrapping the set of expert demonstrations \demoset{} to generate an
unsupervised set of labeled sequences, $\labelset{}_u$, and we refer to this
method as leave-one-out labeling.

For each demonstration $\demovar{} \in \demoset{}$, we generate a corresponding
$\labelvar \in \labelset_u$ as follows. For every other demonstration
$\demovar{}' \neq \demovar{} \in \demoset{}$, we simply compute the TPS
registration cost between $\policysub{\demovar{}'}{d_{pc}}$ and
$\policysub{\demovar{}}{\demovar{}_{pc}}$. We choose the $\demovar{}^*$ with the
lowest registration cost, and add $(\demovar{}_{pc}, \demovar{}^*_{traj})$ to
$\labelset{}_u$ as an expert demonstration. Intuitively, this procedure looks to
discover the expert demonstration $d'$ that, when applied to the start state of
$d$, most closely mimics the optimal choice provided by the expert.

This method of labeling requires slight modifications to our max-margin
formulation. Since the expert's trajectory is assumed to be optimal for its
corresponding state, we exclude each expert demonstration from the max margin
constraints its corresponding unsupervisedexample generates. Additionally, note
that this unsupervised method of generating labeled sequences does not include a
temporal component. Thus, when forming the optimization problem with
$\labelset{}_u$, we must omit Bellman constraints. \et{If time/space permits
  this is where I'd talk about the second round of bootstrapping.}
