\section{Problem Description and Formulation}
\label{sec:formulation}
%\input{variables.tex}

In this section, we will formulate our approach to doing WillSmith, and
specify our assumptions regarding the problems it is applied to.

\subsection{Demonstration Set}

The required inputs of our pipeline are a set \demoset{}
of expert demonstrations and a set \labelset{} of labeled sequences of
task executions (Section~\ref{subsec:labeledex}).
Each \demovar{} $\in$ \demoset{} corresponds to a complete expert-guided
demonstration of the given robotic task or a movement primitive of a complete
demonstration --- our WillSmith formulation assumes the latter,
since it is often necessary for complex, multi-step tasks.
In our application of knot tying, we split each expert demonstration of
tying a knot into three or four movement primitives. The movement primitives
also include demonstration segments that attempt to
recover from failure states \cite{Schulmanetal_ISRR2013}.
Ideally, the segments in \demoset{} contain at least several demonstrations
of each movement primitive in the task.


Each \labelsub{j} $\in$ \labelset{} corresponds to a labeled sequence of
task executions of length \nsub{j}: [\sapairsub{1}, \sapairsub{2}, \ldots,
\sapairsub{\nsub{j}}]. \sh{Not sure if we should talk about \labelset{}
before or after describing MDP, since we need the transition function to
completely specify \labelset{}. Opinions?}\dhm{I think its ok, we'll talk about MDPs 
in the technical background}

A demonstration \demovar{} is composed of (\demosub{pc}, \demosub{traj}),
where \demosub{pc} is a point cloud representation of the underlying
starting state of the demonstration, and \demosub{traj} stores
the trajectory executed by the expert in that demonstration. The trajectory
space \trajset{} spans the set of all possible trajectories.


\subsection{Markov Decision Process Formulation}
Selecting from multiple demonstrations can be framed as a Markov Decision
Process (MDP), which enables us to apply Q-function approximation to derive a
policy.

In order to make planning in this space tractable, we use the set of
demonstrations \demoset{} to define a set of abstract actions \actionset{}. Each
action $\actionvar \in \actionset$ denotes the choice to use a corresponding
policy $\pi_d$, where we apply $\pi_d$ to some state $s$ by transferring
$d_{traj}$ to $s$ via TPS warping, then executing the resulting trajectory.

Since each action $\actionvar \in \actionset$ has a corresponding demo $\demovar
\in \demoset$, from here onward we refer to actions and demos
interchangeably. Additionally, $\policysub{d}{\statevar} =
\policysub{a}{\statevar}$ is most precisely defined as the trajectory that is
found via trajectory transfer from $\demovar$ to $\statevar$. However, since our
transition model is deterministic, applying this trajectory to $\statevar$ will
always produce the same successor $s'$. Thus, for ease of notation we will use
$\pi_{d}$ as the successor function for transferring $d_{traj}$ to a state $s$,
and say that $\policysub{d}{\statevar} = s'$.

Thus, the MDP for an \textsc{mmql} problem is defined as
$\langle\stateset,\goalset,\actionset,T,R,H\rangle$, where

\begin{align*}
\stateset{} &=  \text{all underlying states of the task} \\
\goalset{} &=  \text{all goal states of the task} \\
\actionset{} &= \{\policysub{\demovar{}}{\statevar{}}\ \mid \text{ } \demovar{} \in \demoset{}\} \\
\transitionfn{}(\statevar{}, \actionvar{}, \nextstatevar{}) &=
    \begin{cases}
    1[\policysub{\actionvar{}}{\statevar{}} = \nextstatevar{}] &\text{ if } \statevar{} \not \in \goalset{} \\
    1[\statevar{} = \statevar{}'] &\text{ if } \statevar{} \in \goalset{}
    \end{cases}\\
\rewardfn{}(\statevar{}) &= -1 {[ \statevar{} \not \in \goalset{} ]}. \\
H &= \text{finite horizon}
\end{align*}

In the above specification, \goalset{} denotes the goal set and contains all
states that satisfy the predefined goal. For example, in the knot-tying example,
\goalset{} denotes the set of tied knots.  Our MDP's reward function is then
simply $-1$ for all non-goal states and $0$ for goal states.

\subsection{Labeled Examples}
\label{subsec:labeledex}

As mentioned, one of the required inputs to our pipeline is a set \labelset{}
of labeled sequences of task executions. Each \labelsub{j} $\in$ \labelset{}
corresponds to a labeled sequence of
task executions of length \nsub{j}: \labelsub{j} = [\sapairsub{1},
\sapairsub{2}, \ldots, \sapairsub{\nsub{j}}]. Each labeled sequence corresponds
to a single complete task execution, ordered in the sequence of states and
actions taken. More formally, it is required that for each trajectory
\labelsub{j} $\in$ \labelset{}, for $1 \leq i \leq$ \nsub{j},
\begin{align*}
T(\statesubsup{i}{(j)}, \actionsubsup{i}{(j)}) &= \statesubsup{i+1}{(j)} \text{ for } i < \nsub{j} \\
\text{and } T(\statesubsup{\nsub{j}}{(j)}, \actionsubsup{\nsub{j}}{(j)}) &\in \goalset{}
\end{align*}

\subsection{Max Margin Formulation}


\subsection{Max Margin Q-Learning Formulation}


\subsection{Leave-One-Out Labeling}
\label{subsec:lool}
One major limitation of the formulation we have outlined is its dependence on
the set of labeled sequences \labelset{}. We now propose an alternative method
of bootstrapping the set of expert demonstrations \demoset{} to generate an
unsupervised set of labeled sequences, $\labelset{}_u$, and we refer to this
method as leave-one-out labeling.

For each demonstration $\demovar{} \in \demoset{}$, we generate a corresponding
$\labelvar \in \labelset_u$ as follows. For every other demonstration
$\demovar{}' \neq \demovar{} \in \demoset{}$, we simply compute the TPS
registration cost between $\policysub{\demovar{}'}{d_{pc}}$ and
$\policysub{\demovar{}}{\demovar{}_{pc}}$. We choose the $\demovar{}^*$ with the
lowest registration cost, and add $(\demovar{}_{pc}, \demovar{}^*_{traj})$ to
$\labelset{}_u$ as an expert demonstration. Intuitively, this procedure looks to
discover the expert demonstration $d'$ that, when applied to the start state of
$d$, most closely mimics the optimal choice provided by the expert.

This method of labeling requires slight modifications to our max-margin
formulation. Since the expert's trajectory is assumed to be optimal for its
corresponding state, we exclude each expert demonstration from the max margin
constraints its corresponding unsupervisedexample generates. Additionally, note
that this unsupervised method of generating labeled sequences does not include a
temporal component. Thus, when forming the optimization problem with
$\labelset{}_u$, we must omit Bellman constraints. \et{If time/space permits
  this is where I'd talk about the second round of bootstrapping.}
