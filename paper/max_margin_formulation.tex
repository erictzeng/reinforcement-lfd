\section{Problem Description}
\label{sec:formulation}
%\input{variables.tex}

In this section, we will formulate our approach to doing WillSmith, and
specify our assumptions regarding the problems it is applied to.

\subsection{Demonstration Set}

The required inputs of our pipeline are a set \demoset{}
of expert demonstrations and a set \labelset{} of labeled sequences of
task executions.
Each \demovar{} $\in$ \demoset{} corresponds to a complete expert-guided
demonstration of the given robotic task or a movement primitive of a complete
demonstration --- the latter may be necessary for complex, multi-step tasks.
In our application of knot tying, we split each expert demonstration of
tying a knot into three or four movement primitives. The movement primitives
also include demonstration segments that attempt to
recover from failure states \cite{Schulmanet al_ISRR2013}.
Ideally, the segments in \demoset{} contain at least several demonstrations
of each movement primitive in the task.

A demonstration \demovar{} is composed of (\demosub{pc}, \demosub{traj}),
where \demosub{pc} is a point cloud representation of the underlying
starting state of the demonstration, and \demosub{traj} stores
the trajectory executed by the expert in that demonstration. The trajectory
space \trajset{} spans the set of all possible trajectories.

Each \labelsub{j} $\in$ \labelset{} corresponds to a labeled sequence of
task executions of length \nsub{j}: [\sapairsub{1}, \sapairsub{2}, \ldots,
\sapairsub{\nsub{j}}]. \sh{Not sure if we should talk about \labelset{}
before or after describing MDP, since we need the transition function to
completely specify \labelset{}. Opinions?}

\subsection{Markov Decision Process}
Selecting from multiple demonstrations can be framed as a Markov Decision
Process (MDP), which allows us to learn a policy through applying Q-learning. The structure of our MDP is as follows:
\begin{align*}
\stateset{} &=  \text{all reachable underlying states of task} \\
\actionset{} &= \{\policysub{\demovar{}}{\statevar{}}\ \| \text{ } \demovar{} \in \demoset{}\} \\
\transitionfn{}(\statevar{}, \actionvar{}, \nextstatevar{}) &=
    \begin{cases}
    1[\policysub{\actionvar{}}{\statevar{}} = \nextstatevar{}] \text{ if } \statevar{} \not \in \goalset{} \\
    1[\statevar{} = \statevar{}'] \text{ if } \statevar{} \in \goalset{}
    \end{cases}\\
\rewardfn{}(\statevar{}) &= -1 {[ \statevar{} \not \in \goalset{} ]} \\
\end{align*}

For any \demovar{} $\in$ \demoset{}, \policysub{d}{s} denotes the trajectory
resulting from applying trajectory transfer to \demosub{traj}, using
the non-rigid registration from state \demosub{pc} to \statevar{}.
The policy \policysub{d}{s} belongs to the set of all policies, \policyset{} : \transitionfn{} \text{x} \stateset{} $\rightarrow$
\stateset{}. There is a one-to-one mapping between demonstrations \demovar{}
$\in$ \demoset{} and actions \actionvar{} $\in$ \actionset{}, so to simplify
notation, from this point forwards we will refer to demonstrations and
actions interchangeably.
With this simplification, we can loosely define \policysub{\actionsub{j}}{\statevar{}} as the
state that results from applying trajectory transfer to the corresponding
demonstration \demosub{j}'s trajectory and executing the resulting trajectory
on the state \statevar{}.

The goal set \goalset{} contains all states that satisfy the
predefined goal, i.e. tying a knot.


