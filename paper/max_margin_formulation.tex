\section{Problem Description}
\label{sec:formulation}
%\input{variables.tex}

In this section, we will formulate our approach to doing WillSmith, and
specify our assumptions regarding the problems it is applied to.

\subsection{Demonstration Set}

The required inputs of our WillSmith approach are a set \demoset{}
of expert demonstrations and a set \labelset{} of labeled sequences of
task executions.
Each \demovar{} $\in$ \demoset{} corresponds to a segment
of an expert-guided demonstration of the given robotic task.
\demovar{} is composed of (\demosub{pc}, \demosub{traj}),
where \demosub{pc} is a point cloud representation of the underlying
starting state of the demonstration \demovar{}, and \demosub{traj} characterizes
the trajectory executed by the expert in that demonstration. All possible
trajectories compose the trajectory space \trajset{}.

Ideally, the segments in \demoset{} contain at least several examples
of each movement primitive in the task.
For example, in our application of knot tying, the movement primitives
consist of three main types of rope manipulation and several types of
execution segments that recover from failure states \cite{Schulmanetal_ISRR2013}.

Each \labelsub{j} $\in$ \labelset{} corresponds to a labeled sequence of
task executions of length \nsub{j}: [\sapairsub{1}, \sapairsub{2}, \ldots,
\sapairsub{\nsub{j}}]. \sh{Not sure if we should talk about \labelset{}
before or after describing MDP, since we need the transition function to
completely specify \labelset{}. Opinions?}\dhm{I think its ok, we'll talk about MDPs 
in the technical background}

\subsection{Markov Decision Process}
Selecting from multiple demonstrations can be framed as a Markov Decision
Process (MDP), which allows us to learn a policy through applying Q-learning.
The structure of our MDP is as follows:
\begin{align*}
\stateset{} &=  \text{all reachable underlying states of task} \\
\actionset{} &= \{\policysub{d}{s}\ \| \text{ } \demovar{} \in \demoset{}\} \\
\transitionfn{}(s, a, s') &= \begin{cases}
                           1[\policysub{a}{s} = s'] \text{ if } s \not \in \goalset{} \\
                           1[\statevar{} = \statevar{}'] \text{ if } s \in \goalset{} \end{cases}\\
\rewardfn{}(s) &= -1 {[ \statevar{} \not \in \goalset{} ]} \\
\end{align*}

For any \demovar{} $\in$ \demoset{}, \policysub{d}{s} denotes the trajectory
resulting from applying trajectory transfer from \demosub{traj}, based
on the non-rigid registration from state \demosub{pc} to \statevar{}.
Thus, \policyset{} : \transitionfn{} \text{x} \stateset{} $\rightarrow$
\stateset{}. There is a one-to-one mapping between demonstrations \demovar{}
$\in$ \demoset{} and actions \actionvar{} $\in$ \actionset{}, so to simplify
notation, we will refer to to demonstrations and actions interchangeably.
With this simplification, we can loosely define \policysub{a}{s} as the
state resulting from applying trajectory transfer from the corresponding
demonstration \demovar{}'s trajectory and executing that trajectory on the
state \statevar{}.

The goal set \goalset{} contains all states that satisfy the
predefined goal, i.e. tying a knot.


