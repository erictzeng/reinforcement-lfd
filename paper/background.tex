\section{Technical Background}

\subsection{Trajectory Transfer through Non-Rigid Registration}
Non-Rigid Registration is a method for computing a a function, $f$, to minimize error between landmark points subject to a regularization term.
A regularizer that has seen widespread, and effective, use for registering spatial data is the Thin Plate Spline regularizer\dhm{Cite Wahba, other stuff}.
Given a set of corresponding points $(x_i, y_i)$, we minimize the following objective:
$$\min_f \sum_i ||x_i - y_i||^2 + C\int dx ||D^2(f)||^2_{Frob}.$$
Where $C$ is a hyper-parameter that trades off between correspondance error and increased curvature.
This problem has a finite dimensional solution in terms of basis functions around the correspondance points.
The reader is referred to \dhm{survey on TPS} for more details.

Schulman et al. leveerage this technique to perform trajectory transfer. 
Using a point cloud representation of both scenes, they use the TPS-RPM algorithm to jointy find point correspondances and a registration between them\dhm{Cite CHui}.
TPS-RPM alternates between (1) estimating correspondences between the two scenes' point clouds and (2) fitting the optimal thin plate spline transformation based on these estimated correspondences. 
The result of this procedure is used to warp the path traced by the end effector of the robot in the demonstration
Finally, this warped trajectory is used as a goal for trajectory following in order to find a similar trajectory that satisfies joint limits and collision constraints.
The result of this step is executed, with the hope that the registration will account for changes in the environment but maintain the important aspects of the manipulation.

As a result, we can consider each demonstration as a policy that is parameterized by the environment around the robot.
Given a demonstration state, demonstration trajectory, and current state it will produce a new trajectory to apply to the current state.
This approach has been efffective in generalizing deformable object manipulations to new scenarios and has been explored for knot-tieing and automated suturing applications.
Schulman et al. extend this to use multiple demonstrations by computing the registration for several candidate demonstrations and selecting the option with the least objective value at optimum.
In this paper, we use leverage this technique to generalize individual demonstrations, but describe a novel method for selecting which demonstration to select.
\subsection{Structured Max Margin}
One way to incorporate expert demonstrations into learning in through the use of max-margin learning.
In this setting, we are given a set of labelled examples, \labelset{} (in our case pairings of states and expert demonstrations), and we would 
like to learn a function so that the labelled examples are all ranked highly.
To build a structured margin, we assume a similarity metric on labels, $m$,  so that we can leverage 
similarities and structure inherent to the problem.
The max-margin framework formalizes these desires through the following optimization problem:
\begin{equation}
\begin{aligned}
& \underset{w, \xi}{\text{minimize}}  & & ||w||^2 + C\sum \xi_i\\
& \text{subject to} & &w^\top \phi(\statevar{}_i, \actionvar{}_i) \geq w^\top \phi(\statevar{}_i, \actionvar{}') + m(\statevar{}_i, \actionvar{}_i, \actionvar{}') - \xi_i 
\\&&&\hspace{2.5cm}\forall (\statevar{}_i, \actionvar{}_i) \in \labelset{}, \forall \actionvar{}' \in \actionset{}\setminus \actionvar{}_i
\end{aligned}
\end{equation}


This appraoch has proven useful in many different scenarios where there is structure to an estimation or learning problem.
A notable, and related, example is that of Inverse Reinforcement Learning, where max-margin consrain reward functions that one might learn so that an expert's policy is optimal.
Maximum Margin Planning is one such application\dhm{CITE}. 
In it, the authors use maximum margin constraints for an MDP with known dynamics, but an unknown cost to place constraints on admissable cost functions.
\subsection{Q Function Approximation}
Markov Decision Processes are the formalization of choice for most stochastic sequential decision making problems.
They provide a way to account for stochasticity and there is a large array of techniques that can be used to solve them.
Formally, an undiscounted, finite-horizon, MDP, M, is represented as tuple $\langle\stateset,\actionset,T,R, H\rangle$~\cite{puterman1994}.
$\stateset{}$ is a set of states, that represent different configurations of our world.
$\actionset{}$ is a set of action we can take.
$T:\stateset{} \times \actionset{} \rightarrow \Delta_{\stateset{}}$ is a function that maps a state and action to a probability distribution over next states.
$R:\stateset{}\times \actionset{} \times \stateset{} \rightarrow \mathbb{R}$ is a function that specifies the reward our agent receives as a function of state, action, and next state.
$H$ is a horizon which determines the number of actions we will take in the MDP.
A solution to an MDP is a mapping from each state and horizon to an action.

The solution to an MDP is found by finding a value function, $V^*$, that satisfies the Bellman equations:
$$V(s, h) = \left\{ \begin{array}{cl} \underset{a}{\max} \underset{s'}{\sum} T(s, a, s')[R(s, a, s') + V(s', h-1)] & h >0 \\ 0 & h = 0\end{array}\right.$$
It is sometimes easier to work with the right hand side of this equation, which we call a $Q$-function. Thus, $V^*(s, h) = \underset{a}{\max}Q^*(s, h, a)$.
There are many approaches to finding such a value function, but they require storing a vector that is $O(|\stateset{}|)$, which can be prohibitive in many applications. 


When faced with large state spaces, a common approach is to resort to value (or Q) function approximation~\cite{schweitzer1985generalized}. This can be done with 

