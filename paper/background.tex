\section{Technical Background}

\subsection{Trajectory Transfer through Non-Rigid Registration}
Non-rigid registration computes a function $f$ that minimizes error between landmark points, subject to a regularization term.
A commonly-used, effective method for registering spatial data is the Thin Plate Spline (TPS) regularizer~\cite{Carr_SIGGRAPH2001, Wahba_TPS1990}.
Given a set of correspondence points $(x_i, y_i)$, we minimize the following objective:
$$\min_f \sum_i ||x_i - y_i||^2 + C\int dx ||D^2(f)||^2_{Frob},$$
where $C$ is a hyper-parameter that trades off between correspondance error and increased curvature.
This problem has a finite dimensional solution in terms of basis functions around the correspondence points.

Schulman et al. leverage TPS to perform trajectory transfer~\cite{Schulmanetal_ISRR2013}.
Using a point cloud representation of both scenes, they use a modification of the TPS-RPM algorithm to jointly find point correspondences and a registration between them~\cite{Chui_CVIU2003}.
TPS-RPM alternates between (1) estimating correspondences between the point clouds of two scenes and (2) fitting the optimal TPS transformation based on these estimated scene correspondences. 
The result of this procedure is used to warp the path traced by the end effector of the robot in the demonstration.
Finally, this warped trajectory is used as a goal for trajectory following in order to find a similar trajectory that satisfies joint limits and collision constraints.
The result trajectory is executed, with the hope that the registration will account for changes in the environment but maintain the important aspects of the manipulation.
They extend this method to make use of multiple demonstrations by computing the registration cost to several candidate demonstrations and selecting the demonstration with the lowest registration cost.

With trajectory transfer, we can associate each demonstration with a policy parameterized by the environment around the robot.
Given a demonstration state, demonstration trajectory, and current state, the induced policy will produce a new trajectory to apply to the current state.
This approach has been effective in generalizing deformable object manipulations to new scenarios and has been explored for knot-tying and automated suturing.
In this paper, we leverage trajectory transfer to generalize individual demonstrations, and provide a novel method for selecting a demonstration to generalize, from multiple demonstrations. This enables us to get more bang for our demonstration buck.

\subsection{Structured Max Margin}
One way to incorporate expert demonstrations into learning is through the use of a max-margin framework.
Max-margin approaches find a separating hyperplane between an expert action and all other actions that maximizes the margin.
This is formalized as the following optimization problem, where $m$ is a similarity metric between actions, and $phi$ is a function mapping state-action pairs to feature vectors:
\begin{equation}
\begin{aligned}
& \underset{w, \xi}{\text{minimize}}  & & ||w||^2 + C\sum \xi_i\\
& \text{subject to} & &w^\top \phi(\statevar{}_i, \actionvar{}_i) \geq w^\top \phi(\statevar{}_i, \actionvar{}') + m(\statevar{}_i, \actionvar{}_i, \actionvar{}') - \xi_i 
\\&&&\hspace{2.5cm}\forall (\statevar{}_i, \actionvar{}_i) \in \labelset{}, \forall \actionvar{}' \in \actionset{}\setminus \actionvar{}_i \\
& & &\xi_i \geq 0 \hspace{1.6cm} \forall i
\end{aligned}
\end{equation}

This approach has proven useful in many different scenarios where there is structure to an estimation or learning problem.
A notable, and related, example is that of Inverse Reinforcement Learning, where max-margin constrain reward functions that one might learn so that an expert's policy is optimal.
Maximum Margin Planning is one such application~\cite{Ratliff_ICML06}. 
In it, the authors use maximum margin constraints for an MDP with known dynamics, but an unknown cost to place constraints on admissable cost functions.

%%PA: also, do we really need to introduce the horizon?  It's good to minimize mental notational overhead; I think we can just say there are sink states where the process terminates which have a value of 0 ?  It'd be good to mention that to encode, let's say, tying a knot in a minimal number of segments, we could associate a reward of -1 with every segment execution.
%%PA: do we need the linear program here?  or can we just say that at the solution the Bellman equation is satisfied (and we already have the Bellman equations earlier on); followed by introducing linear function approximation and saying that when using function approximation one goes after weights that make the Bellman equations approximately satisfied; this would cut down the number of equations to one in the section below

\subsection{Markov Decision Processes}
Markov Decision Processes are the formalization of choice for most stochastic sequential decision making problems.
They provide a way to account for stochasticity and there is a large array of techniques that can be used to solve them.
Formally, an undiscounted, finite-horizon, MDP, M, is represented as tuple $\langle\stateset,\actionset,T,R, H\rangle$~\cite{puterman1994}.
$\stateset{}$ is a set of states, that represent different configurations of our world.
$\actionset{}$ is a set of action we can take.
$T:\stateset{} \times \actionset{} \rightarrow \Delta_{\stateset{}}$ is a function that maps a state and action to a probability distribution over next states.
$R:\stateset{}\times \actionset{} \times \stateset{} \rightarrow \mathbb{R}$ is a function that specifies the reward our agent receives as a function of state, action, and next state.
$H$ is a horizon which determines the number of actions we will take in the MDP.
A solution to an MDP is a mapping from each state and horizon to an action.

The solution to an MDP is found by finding a value function, $V^*$, that satisfies the Bellman equations:
$${\footnotesize V(s, h) = \left\{ \begin{array}{cl} \underset{a}{\max}\ \underset{s'}{\sum} T(s, a, s')[R(s, a, s') + V(s', h-1)] & h >0 \\ 0 & h = 0\end{array}\right.}$$
It is sometimes easier to work with the right hand side of this equation, which we call a $Q$-function. Thus, $V^*(s, h) = \underset{a}{\max}Q^*(s, h, a)$.
There are many approaches to finding such a value function, but they require storing a vector that is $O(|\stateset{}|)$, which can be prohibitive in many applications. 


When faced with large state spaces, a common approach is to resort to value (or Q) function approximation~\cite{schweitzer1985generalized}. 
Given a set of basis functions $\phi: \stateset{}\times \actionset{} \rightarrow \mathbb{R}$ we can formulate an approximate linear program:
\begin{equation}
{\footnotesize
\begin{aligned}
\underset{w, \nu}{\min}\ \    &-w_0 + -\sum_{i, h} w^\top \phi(s_i, a_i, h) + C\sum \nu_i\\
\text{s.t.}\ \   &w^\top \phi(\statevar{}_i, \actionvar{}_i, h) \leq \\ &\ \ \ \ \ \ \underset{s'}{\sum}[R(s_i, a_i, s') + T(s_i, a_i, s')\underset{a'}{\max}\ w^\top \phi(\statevar{}'_i, \actionvar{}', h-1)] -  \nu_i \\
&w^\top \phi(\statevar{}_i, \actionvar{}_i, h) \leq 0
\end{aligned}}
\end{equation}
The solution to this optimization is a lower bound on the Q function of the MDP and have many fewer variables than the exact value function computation~\cite{puterman1994}.
However, we will still frequently have too many constraints in order to solve this optimization exactly.
Thus, many approximate linear programming techniques will further approximate by subsampling constraints.
While the result may no longer be an underestimator, contraints samples techniques have been quite successful in solving many large MDP problems.


