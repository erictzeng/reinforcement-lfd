\section{Technical Background}

\subsection{Trajectory Transfer through Non-Rigid Registration}
Non-rigid registration computes a function $f$ that minimizes error between landmark points, subject to a regularization term.
A commonly-used, effective method for registering spatial data is the Thin Plate Spline (TPS) regularizer \dhm{Cite Wahba, other stuff}.
Given a set of correspondence points $(x_i, y_i)$, we minimize the following objective:
$$\min_f \sum_i ||x_i - y_i||^2 + C\int dx ||D^2(f)||^2_{Frob}$$,
%%PA: move "," before the $$ for proper typesetting
where $C$ is a hyper-parameter that trades off between correspondance error and increased curvature.
This problem has a finite dimensional solution in terms of basis functions around the correspondence points.
The reader is referred to \dhm{cite survey on TPS} for more details.

Schulman et al. %%PA: citation missing
leverage TPS to perform trajectory transfer. 
Using a point cloud representation of both scenes, they use the TPS-RPM algorithm to jointy find point correspondances and a registration between them \dhm{Cite CHui}.
TPS-RPM alternates between (1) estimating correspondences between the point clouds of two scenes and (2) fitting the optimal TPS transformation based on these estimated scene correspondences. 
The result of this procedure is used to warp the path traced by the end effector of the robot in the demonstration.
Finally, this warped trajectory is used as a goal for trajectory following in order to find a similar trajectory that satisfies joint limits and collision constraints.
The result trajectory is executed, with the hope that the registration will account for changes in the environment but maintain the important aspects of the manipulation.

%%PA: i'd phrase this a bit differently; rather than considering each demonstration a policy; I'd say something about having an approach to generalize demonstrations to new situations;  hence a single demonstration can be seen as inducing a policy

%%PA: there is a lot of "Schulman et al" here; would be nice to just say that at the beginning of a paragraph stating Schulman et al present the following approach to ... :     and then describe everything in that paragraph;  
As a result, we can consider each demonstration as a policy that is parameterized by the environment around the robot.
Given a demonstration state, demonstration trajectory, and current state, the induced policy will produce a new trajectory to apply to the current state.
This approach has been effective in generalizing deformable object manipulations to new scenarios and has been explored for knot-tying and automated suturing.
Schulman et al. extend this method to make use of multiple demonstrations by computing the registration cost to several candidate demonstrations and selecting the demonstration with the lowest registration cost.
In this paper, we leverage trajectory transfer to generalize individual demonstrations, but describe a novel method for selecting from multiple demonstrations.
%%PA: some motivation in preceding sentence would be good; currently that sentence makes it sound as a really minor increment ... (which it isn't!)

\subsection{Structured Max Margin}
One way to incorporate expert demonstrations into learning in through the use of max-margin learning.
%%PA: in -> is
%%PA: overall this paragraph feels a bit wordy and not exactly paper-writing style "would like to" etc; how about cutting it straight to the point, and lead off with "Max-margin approaches ... " and say what they do in one sentence, and present the optimization problem, and describe what it encodes.
In this setting, we are given a set of labelled examples, \labelset{} (in our case pairings of states and expert demonstrations), and we would 
like to learn a function so that the labelled examples are all ranked highly.
To build a structured margin, we assume a similarity metric on labels, $m$,  so that we can leverage 
similarities and structure inherent to the problem.
The max-margin framework formalizes these desires through the following optimization problem:
\begin{equation}
\begin{aligned}
& \underset{w, \xi}{\text{minimize}}  & & ||w||^2 + C\sum \xi_i\\
& \text{subject to} & &w^\top \phi(\statevar{}_i, \actionvar{}_i) \geq w^\top \phi(\statevar{}_i, \actionvar{}') + m(\statevar{}_i, \actionvar{}_i, \actionvar{}') - \xi_i 
\\&&&\hspace{2.5cm}\forall (\statevar{}_i, \actionvar{}_i) \in \labelset{}, \forall \actionvar{}' \in \actionset{}\setminus \actionvar{}_i
\end{aligned}
\end{equation}


This approach has proven useful in many different scenarios where there is structure to an estimation or learning problem.
A notable, and related, example is that of Inverse Reinforcement Learning, where max-margin constrain reward functions that one might learn so that an expert's policy is optimal.
Maximum Margin Planning is one such application\dhm{CITE}. 
In it, the authors use maximum margin constraints for an MDP with known dynamics, but an unknown cost to place constraints on admissable cost functions.

%%PA: could subsection below be called Markov Decision Processes instead?
%%PA: also, do we really need to introduce the horizon?  It's good to minimize mental notational overhead; I think we can just say there are sink states where the process terminates which have a value of 0 ?  It'd be good to mention that to encode, let's say, tying a knot in a minimal number of segments, we could associate a reward of -1 with every segment execution.
%%PA: do we need the linear program here?  or can we just say that at the solution the Bellman equation is satisfied (and we already have the Bellman equations earlier on); followed by introducing linear function approximation and saying that when using function approximation one goes after weights that make the Bellman equations approximately satisfied; this would cut down the number of equations to one in the section below

\subsection{Q Function Approximation}
Markov Decision Processes are the formalization of choice for most stochastic sequential decision making problems.
They provide a way to account for stochasticity and there is a large array of techniques that can be used to solve them.
Formally, an undiscounted, finite-horizon, MDP, M, is represented as tuple $\langle\stateset,\actionset,T,R, H\rangle$~\cite{puterman1994}.
$\stateset{}$ is a set of states, that represent different configurations of our world.
$\actionset{}$ is a set of action we can take.
$T:\stateset{} \times \actionset{} \rightarrow \Delta_{\stateset{}}$ is a function that maps a state and action to a probability distribution over next states.
$R:\stateset{}\times \actionset{} \times \stateset{} \rightarrow \mathbb{R}$ is a function that specifies the reward our agent receives as a function of state, action, and next state.
$H$ is a horizon which determines the number of actions we will take in the MDP.
A solution to an MDP is a mapping from each state and horizon to an action.

The solution to an MDP is found by finding a value function, $V^*$, that satisfies the Bellman equations:
$${\footnotesize V(s, h) = \left\{ \begin{array}{cl} \underset{a}{\max}\ \underset{s'}{\sum} T(s, a, s')[R(s, a, s') + V(s', h-1)] & h >0 \\ 0 & h = 0\end{array}\right.}$$
It is sometimes easier to work with the right hand side of this equation, which we call a $Q$-function. Thus, $V^*(s, h) = \underset{a}{\max}Q^*(s, h, a)$.
There are many approaches to finding such a value function, but they require storing a vector that is $O(|\stateset{}|)$, which can be prohibitive in many applications. 


When faced with large state spaces, a common approach is to resort to value (or Q) function approximation~\cite{schweitzer1985generalized}. 
Given a set of basis functions $\phi: \stateset{}\times \actionset{} \rightarrow \mathbb{R}$ we can formulate an approximate linear program:
\begin{equation}
{\footnotesize
\begin{aligned}
\underset{w, \nu}{\min}\ \    &-w_0 + -\sum_{i, h} w^\top \phi(s_i, a_i, h) + C\sum \nu_i\\
\text{s.t.}\ \   &w^\top \phi(\statevar{}_i, \actionvar{}_i, h) \leq \\ &\ \ \ \ \ \ \underset{s'}{\sum}[R(s_i, a_i, s') + T(s_i, a_i, s')\underset{a'}{\max}\ w^\top \phi(\statevar{}'_i, \actionvar{}', h-1)] -  \nu_i \\
&w^\top \phi(\statevar{}_i, \actionvar{}_i, h) \leq 0
\end{aligned}}
\end{equation}
The solution to this optimization is a lower bound on the Q function of the MDP and have many fewer variables than the exact value function computation~\cite{puterman1994}.
However, we will still frequently have too many constraints in order to solve this optimization exactly.
Thus, many approximate linear programming techniques will further approximate by subsampling constraints.
While the result may no longer be an underestimator, contraints samples techniques have been quite successful in solving many large MDP problems.


