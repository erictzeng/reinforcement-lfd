\section{Experiments and Results}
\label{sec:experiments}

\subsection{Experimental Setup}
We evaluate the greedy policy and two beam search lookahead policies using the Willow Garage PR2 robot in a simulation environment on the task of tying an overhand knot.

\subsubsection{Demonstrations}
We use the set of demonstrations collected by \citet{Schulmanetal_ISRR2013} for their rope-tying experiments.
These demonstrations were collected in the real world and consist of 148 pairs of point clouds and gripper trajectories.
The point clouds were collected using an RGBD camera and then filtered based on color to remove the green background of the table, and leave only the point cloud representation of the rope.
The gripper trajectories were recorded kinesthetically, from human-guided demonstrations that move the robot's grippers to tie a knot.
The dataset contains demonstrations for a three steps method of tying an overhand knot and recovery demonstrations that
enable recovery from common failures.

\subsubsection{Simulation Environment for Knot Tying} 
The training, validation, and evaluation of our policy are run in simulation.
The simulation environment uses Bullet Physics~\cite{Bullet_Physics} as the physics engine.
The rope is simulated with a linked chain of capsules with bending and torsional constraints.

\subsubsection{Benchmark}
For the labeled examples \labelset{} used to train our \mmql{} model, we distinguish between human-labeled examples 
and automatically generated leave-one-out labeled examples.
In both cases, an example is a pairing of a point cloud and an optimal action.
For the human-labeled examples, we use traces of complete task executions, from a randomly drawn initial 
rope configuration to a successfully tied knot.
Each example contains a reference to its predecessor, so we can represent the associated Bellman equations.

Initial rope states are randomly perturbed configurations of uniformly drawn samples from the initial rope states in the demonstration library.
The process of perturbing a rope state consists of selecting five uniformly-spaced points along the rope and dragging each in a random direction for a random radius between 0 and 15 cm.
All initial rope configurations in our experiments are drawn from this distribution.

Our labeling interface shows the user simulations of applying demonstrations in decreasing order a registration cost, which is a rough measure of the quality of an action.
When human labeler see an action that she deems optimal (or close enough to optimal) she indicates that, the simulated robot commits to that action and the process 
proceeds until a knot is tied.
The leave-one-out labeled examples are generated as outlined in Section~\ref{subsec:lool}. 

In addition, the benchmark contains an evaluation set of 500 randomly drawn initial configurations. 
We define success to be tying an overhand knot in a sequence of 5 or fewer actions.

\subsection{Experiments}
We evaluate the success rate using the following policies when using the expert-labeled and leave-one-out labeled examples: greedy, one-step lookahead with width 10, and two-step lookahead with width 5.
\subsubsection{Expert-labeled Examples}
For this evaluation, 1000 expert-labeled examples are used.
For each of these policies, the optimization hyperparameters $C$, $D$ and $F$ are first tuned via holdout 
validation, using a holdout set of 100 randomly drawn initial rope states (distinct from the evaluation set). 
For this case, the best-performing hyperparameters we found are $C=2$, $D=1$ and $F=1$.
We applied \mmql{} with these hyper-parameter settings and ran each of the three policies 
on the evaluation set.
% TODO: Add in what the hyperparameters are, once we decide on them
As a baseline comparison, we also evaluate using the nearest neighbor policy presented in \citet{Schulmanetal_ISRR2013}.
Their policy chooses the action associated with the state that has the smallest registration cost with respect to the current state.
The success rates obtained under these policies are summarized in Table~\ref{table:performance}. Note that our best results surpass the baseline by 26.4\%.
A little over half of our improvement stems from 
% TODO: Add more analysis of the results, e.g. 1-10 and 2-5 lookahead perform approximately equally well

\begin{table}
  \centering
  \begin{tabular}{lc}
    \toprule
      Policy & Test Accuracy\\
    \midrule
      Nearest neighbor \cite{Schulmanetal_ISRR2013} & 68.8\% \\
    \midrule
      Greedy & 85.6\% \\
      Lookahead (depth 1, width 10) & 93.6\% \\
      Lookahead (depth 2, width 5) & 95.2\% \\
    \bottomrule
  \end{tabular}
  \caption{Success rate of tying a knot using the expert-labeled examples. The neareast-neighbor method selects
           the demonstration that minimizes a bi-directional registration cost associated with the trajectory transfer.
           Other policies maximize the approximate Q-function we learn through \mmql{}. Greedy maximizes this value in the current
           state. The lookahead policies act to maximize a back up value computed by beam search with the specified parameters. The greedy
           succeeds in an additional 17\% of examples when compared with the baseline. Lookahead policies acheive very high performance rates
           and approach the best possible with our demonstration library (\al{fill in upper bound on performance})}
  \label{table:performance}
\end{table}

The approximate values of the states for the 500 evaluated runs in the validation set are plotted in Figure~\ref{fig:values} as a function of time steps 1 through 5.
Observe that Q values are generally increasing; we expect this since the Bellman constraints in our formulation pushes states closer to the goal to have higher values.
Also, recall that the state value of the tied knot is close to zero, which is visible in the figure (Equation~\ref{eq:bellman_goal_constr}).
For most of the runs, the policy is able to reach the goal state in 3 steps, which is optimal since the expert took a minimun of three actions to tie a knot.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\textwidth]{figures/state_value_baseline.pdf}
    \caption{baseline}
    \label{fig:value_baseline}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\textwidth]{figures/state_value_greedy.pdf}
    \caption{greedy}
    \label{fig:value_greedy}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\textwidth]{figures/state_value_lookahead1-10.pdf}
    \caption{lookahead (depth 1, width 10)}
    \label{fig:value_lookahead_1}
  \end{subfigure}
  \begin{subfigure}[b]{0.48\linewidth}
    \includegraphics[width=\textwidth]{figures/state_value_lookahead2-5.pdf}
    \caption{lookahead (depth 2, width 5)}
    \label{fig:value_lookahead_2}
  \end{subfigure}
  \caption{Plots of Q values over executions on the evaluation set. The x-axis indicates which step in the execution
           the evaluation comes from. A value of 0 corresponds to successfully tying a knot. For comparison, we show the 
           negative registration cost of the nearest-neighbor baseline. The structure in our learned function can be seen
           in the strong upward trend in Q-values as execution progresses. The lack of structure in the nearest-neighbor
           motivates the need to Q-function learning: minimizing registration cost in a forward search will \emph{harm} performance.}
\end{figure}

We also examine the relation between the number of expert-labeled examples and the success rate for each of the four policies, including the nearest-neighbor baseline policy.
These success rates are shown in Figure~\ref{fig:number_examples}.
The success rate of the baseline policy is a constant, since the nearest neighbor policy doesn't use the expert-labeled examples.
As expected, the success rate increases as more labeled examples are used. However, these gains exhibit diminishing rewards; after
a certain number of examples performance increases stop.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.9\linewidth]{figures/placeholder.png}
  \caption{Success rates as a function of the number of expert-labelled examples for the four policies. The nearest-neighbor policy is fixed
           an appears as a constant. \mmql{} improves as the amount of data increased. However at a certain point, we approach the limits of this
           method and improvements taper off.}
  \label{fig:number_examples}
\end{figure}

\subsubsection{Leave-One-Out Labeled Examples}

After holdout validation and evaluation, done in the same manner as our main
experiment, we find that using a leave-one-out labeled example set achieves a
performance of XX\%, a drop of XX\% from our supervised result.
%\et{It feels difficult to do further analysis without an actual number here. I'll save this
%for later, if that's alright with everyone.}
